# -*- coding: utf-8 -*-
"""Untitled37.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17B1l2GIi0BwFy2I7wkSHt3y4ZBrJqaQK
"""

import pandas as pd
data = pd.read_json('/content/arc-agi_evaluation_challenges.json')
print(data.head())

# Step 3: Inspect the first few rows to identify any unusual structures
print("Sample data:")
print(data.head())

# Step 4: Check for missing values
print("\nMissing values in each column:")
print(data.isnull().sum())

# Step 5: Inspect data types
print("\nData types of each column:")
print(data.dtypes)

# Step 6: Check for non-hashable columns
# Check if any cell contains a list or dictionary
non_hashable_columns = [col for col in data.columns if data[col].apply(lambda x: isinstance(x, (list, dict))).any()]
print("\nNon-hashable columns:", non_hashable_columns)

# Step 7: Flatten or convert non-hashable columns (if any)
for col in non_hashable_columns:
    # Convert list to string representation or handle dicts if necessary
    data[col] = data[col].apply(lambda x: str(x) if isinstance(x, (list, dict)) else x)

# Step 8: Check for duplicates
duplicates = data.duplicated().sum()
print("\nNumber of duplicate rows:", duplicates)

# Step 9: Drop duplicates if found
if duplicates > 0:
    data = data.drop_duplicates()
    print("Duplicates dropped. New shape:", data.shape)

# Step 10: Handle missing values
# Impute missing values (mean for numerical, mode for categorical)
for column in data.select_dtypes(include=['float64', 'int64']).columns:
    data[column].fillna(data[column].mean(), inplace=True)

for column in data.select_dtypes(include=['object']).columns:
    data[column].fillna(data[column].mode()[0], inplace=True)

# Step 11: Check if there are still missing values
print("\nMissing values after cleaning:")
print(data.isnull().sum())

# Step 12: Final data shape
print("\nFinal shape of the dataset:", data.shape)

cleaned_data_file = 'cleaned_data.csv'  # Change the file name as needed
data.to_csv(cleaned_data_file, index=False)
print(f"Cleaned data saved to '{cleaned_data_file}'.")

# Example: Summary statistics
summary_stats = data.describe(include='all')  # Include='all' to get stats for all columns
print(summary_stats)

# Check the first few rows of the cleaned data
print(data.head())

# Check the columns in the cleaned data
print(data.columns)

# Check the length of outputs before reshaping
print("Number of elements in outputs:", outputs.size)

# Ensure the total size of outputs is divisible by 36 (for a 6x6 shape)
if outputs.size % 36 == 0:
    outputs = outputs.reshape(-1, 6, 6)
else:
    print(f"Cannot reshape array of size {outputs.size} into (6,6).")
    # You might need to trim or adjust your output data here

outputs = outputs.reshape(-1, 20, 20)

import numpy as np

# Example outputs array
outputs = np.random.rand(400)  # Replace this with your actual array

# Desired size for reshaping
desired_size = 36

# Check if trimming or padding is needed
if outputs.size > desired_size:
    # Trim the array to the first 36 elements
    outputs_resized = outputs[:desired_size]
elif outputs.size < desired_size:
    # Pad the array with zeros to reach 36 elements
    pad_size = desired_size - outputs.size
    outputs_resized = np.pad(outputs, (0, pad_size), mode='constant')
else:
    # If the size is already 36, just assign it directly
    outputs_resized = outputs

# Reshape to (6, 6)
outputs_reshaped = outputs_resized.reshape(6, 6)

print("Reshaped Output:", outputs_reshaped.shape)

import matplotlib.pyplot as plt

# Plot the reshaped output as a heatmap
plt.imshow(outputs_reshaped, cmap='viridis', interpolation='nearest')
plt.colorbar()
plt.title('Reshaped Output Heatmap')
plt.show()

# Example: Multiply the reshaped matrix by itself (element-wise)
result = np.dot(outputs_reshaped, outputs_reshaped)
print(result)

# Transpose the matrix
transposed = outputs_reshaped.T
print(transposed)

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# Example data (replace with your actual data)
inputs = np.random.rand(1000, 6, 6, 1)  # Random input grids
outputs = np.random.rand(1000, 6, 6, 1)  # Random output grids

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(inputs, outputs, test_size=0.2, random_state=42)

# Define the CNN model
model = keras.Sequential([
    layers.Input(shape=(6, 6, 1)),  # Use Input layer to specify input shape
    layers.Conv2D(32, (2, 2), activation='relu', padding='same'),  # Adjust kernel size and add padding
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (2, 2), activation='relu', padding='same'),  # Adjust kernel size and add padding
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(36, activation='sigmoid')  # 6x6=36 for output
])

model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train.reshape(-1, 36), epochs=10, validation_data=(X_val, y_val.reshape(-1, 36)))

# Save the model
model.save('arc_model.h5')

# Assuming you have a test dataset named 'X_test'
# Replace this with your actual test dataset
X_test = np.random.rand(200, 6, 6, 1)  # Example test input data

# Load the trained model (if not already in memory)
model = keras.models.load_model('arc_model.h5')

# Make predictions
predictions = model.predict(X_test)

# Reshape predictions to match the expected output format
predictions_reshaped = predictions.reshape(-1, 6, 6, 1)  # Reshape to (n_samples, 6, 6, 1)

from keras.models import load_model
from keras.optimizers import Adam

# Load the trained model
model = load_model('arc_model.h5')

# Recompile the model
model.compile(optimizer=Adam(), loss='mean_squared_error')  # Adjust loss as per your needs

import numpy as np
from keras.models import load_model
from keras.optimizers import Adam
import json

# Load the trained model
model = load_model('arc_model.h5')

# Recompile the model
model.compile(optimizer=Adam(), loss='mean_squared_error')  # Adjust loss function if necessary

# Assuming you have a test dataset named 'X_test'
# Replace this with your actual test dataset
X_test = np.random.rand(200, 6, 6, 1)  # Example test input data

# Make predictions
predictions = model.predict(X_test)

# Reshape predictions to match the expected output format
predictions_reshaped = predictions.reshape(-1, 6, 6, 1)  # Reshape to (n_samples, 6, 6, 1)

# Prepare your submission dictionary
submission = {}
task_ids = ["00576224", "009d5c81", "12997ef3"]  # Example task IDs

for task_id in task_ids:
    submission[task_id] = []

    # For each task, add two attempts
    attempt_1 = predictions_reshaped[0].tolist()  # Replace with actual attempts
    attempt_2 = predictions_reshaped[1].tolist()  # Replace with actual attempts

    submission[task_id].append({
        "attempt_1": attempt_1,
        "attempt_2": attempt_2
    })

# Save the submission to a JSON file
with open('submission.json', 'w') as f:
    json.dump(submission, f, indent=4)

print("Submission file saved as 'submission.json'.")

# Assuming you have the ground truth for your test set
y_test = np.random.rand(200, 6, 6, 1)  # Replace with your actual test labels

# Evaluate the model on test data
loss = model.evaluate(X_test, y_test.reshape(-1, 36))  # Reshape if necessary
print(f"Test Loss: {loss}")